# lang-stats-sp2016
Language &amp; Statistics Project Group

higher order n-grams: http://ceur-ws.org/Vol-377/paper4.pdf

lstms: couldn't find

(what were the other methods we discussed?)



FIRST MEETING IDEAS
------------------

1. Sample from the 100 million words corpus to create more training data
2. Train n-gram models on the 100 million word corpus to find "true" distributions. Then, calculate cross entropy of real and fake articles n-gram distributions and use the value as a feature
3. LSTM after samplinf from the 100 million word corpus
4. Word vectors as features
5. Capture longer-range dependencies that n-grams can manage
